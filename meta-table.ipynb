{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import gspread\n",
    "# from gspread_dataframe import set_with_dataframe\n",
    "# from io import BytesIO\n",
    "# import openpyxl\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get environment variables\n",
    "SERVICE_ACCOUNT_FILE = os.getenv(\"SERVICE_ACCOUNT_FILE\")\n",
    "SCOPES = os.getenv(\"API_SCOPES\").split(\",\")\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE,\n",
    "    scopes=SCOPES\n",
    ")\n",
    "\n",
    "# Setup Drive and Sheets API clients\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_google_sheets_in_first_level_subfolders(drive_service, root_folder_id):\n",
    "    sheets = []\n",
    "\n",
    "    # Step 1: Get first-level sub-folders\n",
    "    subfolders_query = f\"'{root_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'\"\n",
    "    subfolders = drive_service.files().list(\n",
    "        q=subfolders_query,\n",
    "        spaces='drive',\n",
    "        fields='files(id, name)'\n",
    "    ).execute().get(\"files\", [])\n",
    "\n",
    "    print(f\"üìÅ Found {len(subfolders)} subfolders\")\n",
    "\n",
    "    # Step 2: In each subfolder, get Google Sheets\n",
    "    for folder in subfolders:\n",
    "        folder_id = folder['id']\n",
    "        folder_name = folder['name']\n",
    "\n",
    "        sheet_query = f\"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet'\"\n",
    "        sheet_files = drive_service.files().list(\n",
    "            q=sheet_query,\n",
    "            spaces='drive',\n",
    "            fields='files(id, name)'\n",
    "        ).execute().get(\"files\", [])\n",
    "\n",
    "        for sheet in sheet_files:\n",
    "            sheets.append({\n",
    "                \"id\": sheet[\"id\"],\n",
    "                \"name\": sheet[\"name\"],\n",
    "                \"folder\": folder_name,\n",
    "                \"folder_id\": folder_id\n",
    "            })\n",
    "\n",
    "    return sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 30 subfolders\n",
      "‚úÖ Found 33 Google Sheets:\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "DRIVER_ROOT_FOLDER_ID = os.getenv(\"DRIVER_ROOT_FOLDER_ID\")\n",
    "\n",
    "sheets = list_google_sheets_in_first_level_subfolders(\n",
    "    drive_service, DRIVER_ROOT_FOLDER_ID\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Found {len(sheets)} Google Sheets:\")\n",
    "\n",
    "dates = []\n",
    "party_files_counter = 0\n",
    "for s in sheets:\n",
    "    name_arrayed = s[\"name\"].split(\" \")\n",
    "    for _ in range(len(name_arrayed)):\n",
    "        earlyRegistration = re.search(\"◊™◊í◊ï◊ë◊ï◊™\", s[\"name\"])\n",
    "        if earlyRegistration != None:\n",
    "            party_files_counter += 1\n",
    "            # print(f\"- {s['name']} (Folder: {s['folder']}) ‚Üí ID: {s['id']}\")\n",
    "            dates.append(\n",
    "                {\n",
    "                    \"file_name\": s[\"name\"],\n",
    "                    \"folder_name\": s[\"folder\"],\n",
    "                    \"id\": s[\"id\"],\n",
    "                }\n",
    "            )\n",
    "            # Found Google sheet in folder, move to next folder\n",
    "            break\n",
    "\n",
    "\n",
    "# print(f\"‚úÖ Found {party_files_counter} Google Sheets with '◊™◊í◊ï◊ë◊ï◊™' in their names:\")\n",
    "# print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n",
      "Loading data for file\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "CACHED_FILE_NAME = os.getenv(\"CACHED_FILE_NAME\")\n",
    "# Initialize an empty set to store unique column names\n",
    "columns_set = set()\n",
    "\n",
    "# Define the cache file\n",
    "CACHE_FILE = os.path.join(os.getcwd(), CACHED_FILE_NAME)\n",
    "\n",
    "# Load the cache if it exists\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# Iterate through all files in the dates object\n",
    "for sheet_info in dates:\n",
    "    file_id = sheet_info[\"id\"]\n",
    "    folder_name = sheet_info[\"folder_name\"]\n",
    "\n",
    "    # Check if the file is already cached\n",
    "    if folder_name in cache and file_id in cache[folder_name]:\n",
    "        print(f\"Loading data for file\") # '{sheet_info['folder_name']}' from cache.\")\n",
    "        data = cache[folder_name][file_id]\n",
    "    else:\n",
    "        print(f\"Fetching data for file\") # '{sheet_info['folder_name']}' from Google Sheets.\")\n",
    "\n",
    "        # Open the Google Sheet by ID\n",
    "        sheet = gc.open_by_key(file_id)\n",
    "\n",
    "        # Access the first worksheet\n",
    "        worksheet = sheet.get_worksheet(0)  # 0 is the index of the first worksheet\n",
    "\n",
    "        # Fetch all rows at once (including headers)\n",
    "        all_rows = worksheet.get_all_values()  # Single API call to fetch all data\n",
    "\n",
    "        # Extract headers from the first row\n",
    "        headers = all_rows[0]  # First row is assumed to be the header row\n",
    "        # print(f\"Headers in sheet '{sheet_info['folder_name']}': {headers}\")\n",
    "\n",
    "        # Extract data from the remaining rows\n",
    "        data = [dict(zip(headers, row)) for row in all_rows[1:]]  # Map rows to headers\n",
    "\n",
    "        # Cache the data\n",
    "        if folder_name in cache:\n",
    "            cache[folder_name][file_id] = data\n",
    "        else:\n",
    "            cache[folder_name] = {file_id: data}\n",
    "\n",
    "        # Iterate through the data list\n",
    "        for row in data:\n",
    "            # Add all keys (column names) from the current dictionary to the set\n",
    "            columns_set.update(row.keys())\n",
    "\n",
    "        # Save the updated cache to the file\n",
    "        with open(CACHE_FILE, \"w\") as f:\n",
    "            json.dump(cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Print the unique column names\n",
    "# print(\"Unique columns across all sheets:\", columns_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached sheets data\n",
    "with open(CACHE_FILE, \"r\") as f:\n",
    "    cached_sheets = json.load(f)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Iterate through each file ID and its corresponding data\n",
    "for folder_name, files in cached_sheets.items():\n",
    "    for file_id, rows in files.items():\n",
    "        # Flatten the rows for the current file ID\n",
    "        df = pd.json_normalize(rows)\n",
    "        df[\"file_id\"] = file_id  # Add a column for the file ID\n",
    "        df[\"folder_name\"] = folder_name  # Add a column for the file ID\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Remove columns with empty or whitespace-only headers\n",
    "final_df = final_df.loc[:, ~(final_df.columns.str.strip() == \"\")]\n",
    "\n",
    "# Remove columns where all values are either NaN or empty strings\n",
    "final_df = final_df.loc[:, ~(final_df.isna() | (final_df == '')).all(axis=0)]\n",
    "\n",
    "def merge_and_rename(content, new_column_name, df):\n",
    "    # Identify columns that contain the word \"◊©◊ù\"\n",
    "    columns_containing = [col for col in df.columns if content in col.lower()]\n",
    "    # Merge the columns into a single column named \"full_name\"\n",
    "    df[new_column_name] = df[columns_containing].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    df = df.drop(columns=columns_containing)\n",
    "    return df\n",
    "\n",
    "\n",
    "## NAME\n",
    "final_df = merge_and_rename(\"◊©◊ù\", \"full_name\", final_df)\n",
    "\n",
    "## SPECIAL REQUESTS\n",
    "final_df = merge_and_rename(\"◊î◊¢◊®◊ï◊™\", \"special_requests\", final_df)\n",
    "\n",
    "## SONGS REQUESTS\n",
    "final_df = merge_and_rename(\"song\", \"song_requests\", final_df)\n",
    "\n",
    "# Rename columns to English\n",
    "final_df = final_df.rename(columns={\n",
    "    \"◊ó◊ï◊™◊û◊™ ◊ñ◊û◊ü\": \"timestamp\",\n",
    "    \"◊î◊í◊ô◊¢.◊î?\": \"arrived\",\n",
    "})\n",
    "\n",
    "\n",
    "##ARRIVED\n",
    "# Step 1: Make sure empty fields are treated correctly\n",
    "final_df['arrived'] = final_df['arrived'].replace('', pd.NA)\n",
    "\n",
    "# Step 2: Update 'arrived_marker' directly\n",
    "final_df.loc[(final_df['arrived'].notna()) & (final_df['arrived_marker'] == '#ffffff'), 'arrived'] = True\n",
    "\n",
    "# Step 3: Update 'arrived' True for colored cells\n",
    "final_df.loc[final_df['arrived_marker'] != \"#ffffff\", 'arrived'] = True\n",
    "\n",
    "# Step 4: Update 'arrived' False if has white background\n",
    "final_df.loc[(final_df['arrived_marker'] == '#ffffff'), 'arrived'] = False\n",
    "\n",
    "##FULL_NAME\n",
    "# Step 1: Remove leading and trailing whitespace\n",
    "final_df['full_name'] = final_df['full_name'].str.strip()\n",
    "\n",
    "#Step 2: Filter out rows with empty 'full_name'\n",
    "final_df = final_df[final_df['full_name'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOP 10 REGISTRATIONS\n",
    "top_registrations = (\n",
    "    final_df.groupby(\"full_name\")\n",
    "    .size()  # faster and lighter than count() here\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# pd.DataFrame(top_registrations, index=None).rename(columns={0: \"Top Registrations\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOP 10 ARRIVALS & REGISTRATIONS\n",
    "top_arrivals = (\n",
    "    final_df[final_df['arrived'] == True]\n",
    "    .groupby('full_name')\n",
    "    .size()  # faster and lighter than count() here\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "# pd.DataFrame(top_arrivals, index=None).rename(columns={0: \"Top Arrived and Registered\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by arrived and count of full name\n",
    "# final_df[['arrived', 'full_name']].groupby(\"arrived\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/k79211k13lqcyqdk75q92xq00000gn/T/ipykernel_65397/1438058289.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  arrived_pivot = arrived_pivot.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Transpose the DataFrame by swapping names in rows and folder names in columns\n",
    "\n",
    "# Step 2: Pivot the DataFrame\n",
    "arrived_pivot = final_df.pivot_table(\n",
    "    index='full_name',  # Use 'full_name' as the row index\n",
    "    columns='folder_name',  # Use 'folder_name' as the columns\n",
    "    values='arrived',  # Use 'arrived' as the values\n",
    "    aggfunc='any'  # Aggregate using 'any' to ensure boolean values\n",
    ")\n",
    "\n",
    "# Step 3: Reset the index (optional)\n",
    "arrived_pivot = arrived_pivot.reset_index()\n",
    "\n",
    "# Step 4: Rename columns (optional)\n",
    "arrived_pivot.columns.name = None  # Remove the column grouping name\n",
    "arrived_pivot = arrived_pivot.rename_axis(None, axis=1)  # Remove the index name\n",
    "\n",
    "# Replace NaN values with False\n",
    "arrived_pivot = arrived_pivot.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrived_pivot.to_csv(\"arrived_pivot.csv\", index=False)\n",
    "\n",
    "# duplicates = final_df[final_df[\"full_name\"].duplicated(keep=False)]\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODOS ###\n",
    "# TODO: ML Bag of words for the song requests (For fun)\n",
    "# TODO: Sentiment analysis for the special requests\n",
    "# TODO: Check how many of the audience are returning and how many are new (each party and overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
