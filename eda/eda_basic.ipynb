{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/k79211k13lqcyqdk75q92xq00000gn/T/ipykernel_39498/3042254910.py:13: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  party_df = pd.read_csv(\"../data/processed/df.csv\", parse_dates=[\"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic Exploratory Data Analysis (EDA)\n",
    "for Party Aggregator project.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "party_df = pd.read_csv(\"../data/processed/df.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "def contains_hebrew(text):\n",
    "    return bool(re.search(r'[\\u0590-\\u05FF]', text))\n",
    "\n",
    "def reverse_if_hebrew(text):\n",
    "    return text[::-1] if contains_hebrew(text) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/k79211k13lqcyqdk75q92xq00000gn/T/ipykernel_39498/2678336164.py:9: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  party_df[\"timestamp\"] = pd.to_datetime(party_df[\"timestamp\"], errors=\"coerce\")  # coerce bad formats to NaT\n"
     ]
    }
   ],
   "source": [
    "# print(f\"df shape: {df.shape}\")\n",
    "# print(f\"df columns: {df.columns}\")\n",
    "# print(f\"df dtypes: {df.dtypes}\")\n",
    "\n",
    "# 1. Remove duplicate rows\n",
    "party_df = party_df.drop_duplicates()\n",
    "\n",
    "# 2. Convert timestamp to datetime\n",
    "party_df[\"timestamp\"] = pd.to_datetime(party_df[\"timestamp\"], errors=\"coerce\")  # coerce bad formats to NaT\n",
    "\n",
    "# 3. Normalize string columns\n",
    "string_cols = [\"arrived_marker\", \"file_id\", \"folder_name\", \"full_name\", \"special_requests\", \"song_requests\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    party_df[col] = party_df[col].astype(str).str.strip()  # remove leading/trailing spaces\n",
    "    party_df[col] = party_df[col].str.replace(r\"\\s+\", \" \", regex=True)  # replace multiple spaces with single space\n",
    "\n",
    "party_df[\"arrived\"] = party_df[\"arrived\"].astype(\"boolean\")  # Nullable BooleanDtype\n",
    "\n",
    "# 4. Replace blank strings with NaN in relevant text fields\n",
    "party_df[[\"special_requests\", \"song_requests\"]] = party_df[[\"special_requests\", \"song_requests\"]].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "\n",
    "# 5. Convert folder names to categorical\n",
    "party_df[\"folder_name\"] = party_df[\"folder_name\"].astype(\"category\")\n",
    "\n",
    "# print(f\"df dtypes: {df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# party_df[['arrived_marker', 'full_name']].groupby(['arrived_marker']).count()\n",
    "# party_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_df[\"song_requests_clean\"] = (\n",
    "    party_df[\"song_requests\"]\n",
    "    .replace([\"\", \" \", \"NA\", \"NaN\", \"nan\", \"n/a\", None], pd.NA)\n",
    "    .astype(\"string\")  # nullable string dtype\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_request_completion(\n",
    "    df, request_col=\"song_requests\", folder_col=\"folder_name\", top_n=None, title=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a stacked bar chart showing percent-filled and percent-empty requests per folder.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        request_col (str): Column with the requests (e.g., 'song_requests').\n",
    "        folder_col (str): Column indicating folder/party (e.g., 'folder_name').\n",
    "        top_n (int, optional): Show only top-N parties by submission count.\n",
    "        title (str, optional): Custom title for the plot.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Aggregation logic ===\n",
    "    g_df = (\n",
    "        df.groupby(folder_col)\n",
    "        .agg(\n",
    "            total_requests=(request_col, \"count\"),\n",
    "            non_empty_requests=(request_col, lambda x: x.astype(str).str.strip().ne(\"\").sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Percent calculations\n",
    "    g_df[\"percent_filled\"] = (g_df[\"total_requests\"] / g_df[\"non_empty_requests\"]) * 100\n",
    "    g_df[\"percent_empty\"] = 100 - g_df[\"percent_filled\"]\n",
    "    g_df[\"folder_name_display\"] = g_df[folder_col].apply(reverse_if_hebrew)\n",
    "\n",
    "    if top_n:\n",
    "        g_df = g_df.sort_values(\"non_empty_requests\", ascending=False).head(top_n)\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars1 = plt.bar(\n",
    "        g_df[\"folder_name_display\"],\n",
    "        g_df[\"percent_filled\"],\n",
    "        label=\"Non-empty (%)\",\n",
    "        color=\"skyblue\",\n",
    "    )\n",
    "    bars2 = plt.bar(\n",
    "        g_df[\"folder_name_display\"],\n",
    "        g_df[\"percent_empty\"],\n",
    "        bottom=g_df[\"percent_filled\"],\n",
    "        label=\"Empty (%)\",\n",
    "        color=\"lightcoral\",\n",
    "    )\n",
    "\n",
    "    # Dynamic labels inside bars\n",
    "    for i in range(len(g_df)):\n",
    "        filled = g_df.loc[i, \"percent_filled\"]\n",
    "        empty = g_df.loc[i, \"percent_empty\"]\n",
    "        x = bars1[i].get_x() + bars1[i].get_width() / 2\n",
    "\n",
    "        if filled > 5:\n",
    "            plt.text(\n",
    "                x,\n",
    "                filled / 2,\n",
    "                f\"{filled:.0f}%\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=8,\n",
    "                color=\"black\",\n",
    "            )\n",
    "        if empty > 5:\n",
    "            plt.text(\n",
    "                x,\n",
    "                filled + empty / 2,\n",
    "                f\"{empty:.0f}%\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=8,\n",
    "                color=\"white\",\n",
    "            )\n",
    "\n",
    "    # Final touches\n",
    "    plt.title(title or f\"{request_col.replace('_', ' ').title()} Completion per Party\")\n",
    "    plt.xlabel(\"Party Name\")\n",
    "    plt.ylabel(\"Request Completion (%)\")\n",
    "    plt.ylim(0, 110)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_request_completion(party_df, request_col=\"song_requests\", title=\"Song Requests Completion per Party\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "he_stop_words_df = pd.read_csv(\"../top_3000_most_freq_wiki.csv\", header=None)\n",
    "hebrew_stop_words = set(he_stop_words_df[1].astype(str).str.strip())\n",
    "\n",
    "# Combine English and Hebrew stop words\n",
    "en_stop_words = set(get_stop_words('en'))\n",
    "he_stop_words = set(hebrew_stop_words)\n",
    "all_stop_words = en_stop_words.union(he_stop_words)\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    # protect against missing/NA values\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # Tokenize with regex to support Hebrew + English\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n",
    "    # Keep only non-stop words and alphabetic tokens\n",
    "    return [t for t in tokens if t not in all_stop_words and t.isalpha() and len(t) > 2]\n",
    "\n",
    "party_df[\"song_request_tokens\"] = party_df[\"song_requests_clean\"].apply(tokenize_and_filter)\n",
    "\n",
    "# df[[\"song_requests\", \"song_requests_clean\", \"song_request_tokens\"]].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams Utility Functions (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Get global token frequency from flattened list ===\n",
    "def get_global_token_freq(df: pd.DataFrame, token_col: str = \"song_request_tokens\", top_n: int = 50) -> pd.DataFrame:\n",
    "    all_tokens = [token for tokens in df[token_col] for token in tokens]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    return pd.DataFrame(token_counts.most_common(top_n), columns=[\"token\", \"count\"])\n",
    "\n",
    "# === Step 2: Plot global token frequency (with RTL support) ===\n",
    "def plot_top_tokens(freq_df: pd.DataFrame, top_n: int = 10, title: str = \"Top Song Request Tokens\") -> None:\n",
    "    freq_df = freq_df.copy()\n",
    "    freq_df[\"token\"] = freq_df[\"token\"].apply(reverse_if_hebrew)\n",
    "    top_df = freq_df.nlargest(top_n, \"count\")\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(top_df[\"token\"], top_df[\"count\"], color=\"orchid\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Token\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === Step 3: Get per-party token frequency table ===\n",
    "def get_party_token_freq(df: pd.DataFrame, folder_col: str = \"folder_name\", token_col: str = \"song_request_tokens\") -> pd.DataFrame:\n",
    "    party_token_counts = defaultdict(Counter)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        party = row[folder_col]\n",
    "        tokens = row[token_col]\n",
    "        party_token_counts[party].update(tokens)\n",
    "\n",
    "    rows = []\n",
    "    for party, counts in party_token_counts.items():\n",
    "        for token, count in counts.items():\n",
    "            rows.append({\"party\": party, \"token\": token, \"count\": count})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# === Step 4: Get top N tokens per party (optional) ===\n",
    "def get_top_tokens_by_party(df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "    return (\n",
    "        df.sort_values([\"party\", \"count\"], ascending=[True, False])\n",
    "        .groupby(\"party\")\n",
    "        .head(top_n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Request Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_freq_df = get_global_token_freq(party_df, top_n=50)\n",
    "# plot_top_tokens(global_freq_df, top_n=30, title=\"Top 30 Global Song Request Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Request Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "party_df[\"bigrams\"] = party_df[\"song_request_tokens\"].apply(lambda tokens: generate_ngrams(tokens, 2))\n",
    "party_df[\"trigrams\"] = party_df[\"song_request_tokens\"].apply(lambda tokens: generate_ngrams(tokens, 3))\n",
    "\n",
    "# Get global top bigrams\n",
    "global_bigram_df = get_global_token_freq(party_df, token_col=\"bigrams\", top_n=30)\n",
    "\n",
    "# Plot\n",
    "# plot_top_tokens(global_bigram_df, top_n=20, title=\"Top 20 Global Bigrams in Song Requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_df[\"special_requests_clean\"] = (\n",
    "    party_df[\"special_requests\"]\n",
    "    .replace([\"\", \" \", \"NA\", \"NaN\", \"nan\", \"n/a\", None], pd.NA)\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "party_df[\"special_request_tokens\"] = party_df[\"special_requests_clean\"].apply(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special requests Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_unigram_df = get_global_token_freq(party_df, token_col=\"special_request_tokens\", top_n=50)\n",
    "# plot_top_tokens(special_unigram_df, top_n=30, title=\"Top Special Request Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special requests Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_df[\"special_bigrams\"] = party_df[\"special_request_tokens\"].apply(lambda tokens: generate_ngrams(tokens, 2))\n",
    "special_bigram_df = get_global_token_freq(party_df, token_col=\"special_bigrams\", top_n=30)\n",
    "# plot_top_tokens(special_bigram_df, top_n=20, title=\"Top Special Request Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_request_completion(party_df, request_col=\"special_requests\", title=\"Special Request Completion per Party\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snap_table",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
